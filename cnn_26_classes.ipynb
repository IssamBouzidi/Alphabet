{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('nuage_compta': conda)",
   "metadata": {
    "interpreter": {
     "hash": "73e9e122b3139777442fddc8a360e9c1c189071e738be63893006cf6b17e3c9d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), input_shape=(28,28,3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters = 32, kernel_size = (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=128, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=26, activation=\"softmax\")) # valeurs de units represente le nombre de valeurs de sortie, ici, c'est 26 nombre de lettre de l'alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 372451 images belonging to 26 classes.\n",
      "Found 372451 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory=\"data/TRAINING\", target_size=(28, 28), batch_size=32, class_mode=\"categorical\")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(directory=\"data/TEST\", target_size=(28,28), batch_size=32, class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/25\n",
      "63/63 [==============================] - 44s 695ms/step - loss: 2.6681 - accuracy: 0.2450 - val_loss: 2.1111 - val_accuracy: 0.3516\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 41s 650ms/step - loss: 2.0176 - accuracy: 0.4355 - val_loss: 1.5557 - val_accuracy: 0.4844\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 43s 680ms/step - loss: 1.7278 - accuracy: 0.5050 - val_loss: 1.4457 - val_accuracy: 0.5840\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 43s 688ms/step - loss: 1.5150 - accuracy: 0.5446 - val_loss: 1.4395 - val_accuracy: 0.6016\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 39s 617ms/step - loss: 1.3454 - accuracy: 0.5655 - val_loss: 0.9200 - val_accuracy: 0.6680\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 47s 754ms/step - loss: 1.2907 - accuracy: 0.6002 - val_loss: 0.8327 - val_accuracy: 0.6602\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 44s 701ms/step - loss: 1.1581 - accuracy: 0.6300 - val_loss: 1.0614 - val_accuracy: 0.7070\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 42s 667ms/step - loss: 1.0570 - accuracy: 0.6652 - val_loss: 0.5692 - val_accuracy: 0.7207\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 43s 683ms/step - loss: 0.9997 - accuracy: 0.6716 - val_loss: 0.6760 - val_accuracy: 0.7461\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 42s 674ms/step - loss: 0.9878 - accuracy: 0.6736 - val_loss: 0.7825 - val_accuracy: 0.7480\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 46s 738ms/step - loss: 0.9563 - accuracy: 0.6900 - val_loss: 0.5849 - val_accuracy: 0.8145\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 43s 685ms/step - loss: 0.9151 - accuracy: 0.7014 - val_loss: 0.6229 - val_accuracy: 0.7734\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 43s 687ms/step - loss: 0.8370 - accuracy: 0.7346 - val_loss: 0.5271 - val_accuracy: 0.7969\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 50s 793ms/step - loss: 0.8360 - accuracy: 0.7381 - val_loss: 0.4580 - val_accuracy: 0.8086\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 44s 695ms/step - loss: 0.8372 - accuracy: 0.7411 - val_loss: 0.4425 - val_accuracy: 0.8105\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 39s 615ms/step - loss: 0.8117 - accuracy: 0.7297 - val_loss: 0.6286 - val_accuracy: 0.8320\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 41s 648ms/step - loss: 0.7711 - accuracy: 0.7455 - val_loss: 0.4307 - val_accuracy: 0.7969\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 43s 684ms/step - loss: 0.7628 - accuracy: 0.7520 - val_loss: 0.4378 - val_accuracy: 0.8594\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - 43s 690ms/step - loss: 0.7100 - accuracy: 0.7723 - val_loss: 0.5135 - val_accuracy: 0.8809\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - 45s 711ms/step - loss: 0.7077 - accuracy: 0.7857 - val_loss: 0.7379 - val_accuracy: 0.8516\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - 45s 720ms/step - loss: 0.6822 - accuracy: 0.7862 - val_loss: 0.4718 - val_accuracy: 0.8613\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - 45s 718ms/step - loss: 0.6977 - accuracy: 0.7798 - val_loss: 0.1833 - val_accuracy: 0.8789\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - 43s 681ms/step - loss: 0.6448 - accuracy: 0.8041 - val_loss: 0.2128 - val_accuracy: 0.8633\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - 47s 748ms/step - loss: 0.6675 - accuracy: 0.7902 - val_loss: 0.3563 - val_accuracy: 0.8672\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - 46s 722ms/step - loss: 0.6062 - accuracy: 0.8165 - val_loss: 0.3296 - val_accuracy: 0.8887\n"
     ]
    }
   ],
   "source": [
    "entrainement = model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=63, # nombre d'image entrainée par lot ==> nombre d'images souhaitées / nombre de lot, ici on suppose on veut entrainé 1000 donc 2000 / 32 (batch_size)\n",
    "    epochs=25, \n",
    "    validation_data=test_generator, \n",
    "    validation_steps=16 # nombre d'image tesé par lot ==> nombre d'images souhaitées / nombre de lot, ici on suppose on veut testé 400 donc 600 / 32 (batch_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarder le model\n",
    "model.save('alphabet_model_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du tableau des alphabets\n",
    "alphabet_array = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "\n",
    "choix = r'data/VALIDATION/B-8460.png'\n",
    "test_model = load_model('alphabet_model.h5')\n",
    "\n",
    "test_image = image.load_img(choix, target_size = (28, 28))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "train_generator.class_indices\n",
    "\n",
    "preds = test_model.predict_classes(test_image)\n",
    "prob = test_model.predict_proba(test_image)\n",
    "\n",
    "index = preds[0]\n",
    "print(f'Il s\\'agit de la lettre \"{alphabet_array[index]}\".')"
   ]
  }
 ]
}